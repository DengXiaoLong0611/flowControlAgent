import gym
import numpy as np
from stable_baselines3 import SAC
import torch
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
from flow_env import myEnv_flow
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import time
import pickle

# run ='train'
# run ='continue_train'
# run ='update'
run='eval'
# run='merge'
# run='newinject'
# æ³¨æ„ç»§ç»­è®­ç»ƒæ—¶æ›´æ”¹æ–‡ä»¶ç›®å½•ï¼Œæ›´æ”¹numstepï¼Œæ‰¾åˆ°ç›®å½•ä¸‹æœ€æ–°çš„stepï¼Œæ”¹æˆå¯¹åº”çš„å€¼
num_steps = 600
load_model_path = 'logs_norm/rl_model_norm_{}_steps.zip'.format(num_steps)
load_buffer_path = 'logs_norm/rl_model_norm_replay_buffer_{}_steps.pkl'.format(num_steps)
# load_normalize_env_path = 'logs_norm/rl_model_norm_vecnormalize_{}_steps.pkl'.format(num_steps)




if run == 'train':
    # è®¾ç½®10æ­¥ä¸€ä¸ªè®­ç»ƒä¿å­˜ç‚¹
    checkpoint_callback = CheckpointCallback(
        save_freq=20,
        save_path="./logs_norm/",
        name_prefix="rl_model_norm",
        save_replay_buffer=True,
        save_vecnormalize=True,  # ä¿å­˜VecNormalizeè®¾ç½®
    )
    callback = [checkpoint_callback]

    # åˆ›å»ºç¯å¢ƒ
    env = DummyVecEnv([lambda: myEnv_flow()])
    # æŒ‡å®šç­–ç•¥ç½‘ç»œçš„ç»“æ„å’Œæ¿€æ´»å‡½æ•°
    policy_kwargs = dict(activation_fn=torch.nn.ReLU,net_arch=dict(pi=[256, 256, 256], qf=[256, 256, 256]))

    # æ¨¡å‹å‚æ•°
    model = SAC("MlpPolicy", 
                env=env, 
                verbose=1,
                policy_kwargs=policy_kwargs,
                batch_size=1536,
                learning_rate=0.001,
                buffer_size=100000,
                learning_starts=0,
                train_freq=(1, 'step'),
                seed=0,
                ent_coef='auto_0.5',
                target_entropy=-5.0,
                gradient_steps=10
                )
    
    #åŠ è½½ replay bufferç»¼åˆè®­ç»ƒï¼Œè®°å¾—ä¿®æ”¹bufferé‡‡æ ·
    replay_buffer_path = "logs_norm/merged_replay_buffer.pkl"  # ä½ çš„ç¼“å†²æ± æ–‡ä»¶è·¯å¾„
    with open(replay_buffer_path, "rb") as f:
        model.replay_buffer = pickle.load(f)

    print(model.actor)
    print(model.critic)
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(total_timesteps=10000, log_interval=1, callback=callback)

    # ä¿å­˜æ¨¡å‹å’ŒVecNormalize
    model.save("SAC_fin")

if run == 'continue_train':
  checkpoint_callback = CheckpointCallback(
    save_freq=20,
    save_path="./logs_norm/",
    name_prefix="rl_model_norm",
    save_replay_buffer=True,
    save_vecnormalize=True,
  )
  callback=[checkpoint_callback]

  env = DummyVecEnv([lambda: myEnv_flow()])
  policy_kwargs = dict(activation_fn=torch.nn.ReLU,net_arch=dict(pi=[256, 256, 256], qf=[256, 256, 256]))
  # ä»ç›®å½•ä¸­åŠ è½½è®­ç»ƒè¿‡çš„æ¨¡å‹ï¼Œå¯é€‚å½“ä¿®æ”¹ç»éªŒæ± ï¼Œå­¦ä¹ ç‡ç­‰
  model = SAC.load(load_model_path,
                   env=env,
                   learning_rate=0.001,
                   action_noise=None,
                   batch_size=1536,
                   learning_starts = 0,
                   ent_coef='auto',
                   target_entropy=-8.0, 
                   gradient_steps=10,
                   train_freq=(5, 'step'),
                   )
  
  # è¯»å–ç»éªŒç¼“å†²æ± ï¼Œå¸®åŠ©ç­–ç•¥è®­ç»ƒ
  model.load_replay_buffer(load_buffer_path)

  print(model.actor)
  print(model.critic) 
  print(model.batch_size)
  print(model.replay_buffer.size())
  print(model.learning_rate)
  print(model._n_updates)
  print(model.gradient_steps)
  model.learn(total_timesteps=10000, log_interval=1, callback=callback, reset_num_timesteps= False)
  model.save("SAC_fin")

if run == 'eval':
  model_path=f"logs_norm/rl_model_norm_1200_steps.zip"
  env = DummyVecEnv([lambda: myEnv_flow()])
  model = SAC.load(model_path,env = env)
  t1 = time.time()
  obs = env.reset()
  j=0
  # å¾ªç¯æ§åˆ¶è¯„ä¼°10æ­¥
  while True:
    j=j+1
    t2 = time.time()
    print('å½“å‰å·²è€—è´¹æ—¶é—´',t2-t1)
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    t3 = time.time()
    print('å½“å‰STEPè€—è´¹æ—¶é—´',t3-t2)
    if j==39:
      break

if run == 'update':
    def load_replay_buffer(file_path):
        """åŠ è½½æŒ‡å®šçš„ ReplayBuffer"""
        with open(file_path, "rb") as f:
            return pickle.load(f)

    def save_replay_buffer(replay_buffer, file_path):
        """ä¿å­˜ ReplayBuffer"""
        updated_path = file_path.replace(".pkl", "_updated.pkl")  # ç”Ÿæˆæ–°æ–‡ä»¶å
        with open(updated_path, "wb") as f:
            pickle.dump(replay_buffer, f)
        print(f"âœ… ReplayBuffer å·²æˆåŠŸä¿å­˜åˆ°: {updated_path}")

    def print_latest_samples(replay_buffer, num_samples=3):
        """æ‰“å° ReplayBuffer ä¸­æœ€æ–°çš„ num_samples ç»„æ•°æ®"""
        max_index = min(num_samples, replay_buffer.size())  # ç¡®ä¿ä¸ä¼šè¶…å‡ºç¼“å†²æ± æ•°æ®
        print(f"\nğŸ“Œ å½“å‰ ReplayBuffer å­˜å‚¨æ•°é‡: {replay_buffer.size()}")
        print(f"ğŸ“Œ å½“å‰å­˜å‚¨ä½ç½®: {replay_buffer.pos}")
        print(f"ğŸ“Œ æ˜¯å¦å·²æ»¡: {replay_buffer.full}")

        start_index = max(replay_buffer.pos - max_index, 0)  # è®¡ç®—æœ€æ–°æ•°æ®çš„èµ·å§‹ç´¢å¼•
        for i in range(start_index, replay_buffer.pos):
            print(f"\nğŸŸ¢ æ ·æœ¬ç´¢å¼• {i}:")
            print(f"ğŸ”¹ çŠ¶æ€ (obs): \n{replay_buffer.observations[i]}")
            print(f"ğŸ”¸ åŠ¨ä½œ (action): \n{replay_buffer.actions[i]}")
            print(f"â­ å¥–åŠ± (reward): \n{replay_buffer.rewards[i]}")
            print(f"â›” æ˜¯å¦ç»ˆæ­¢ (done): \n{replay_buffer.dones[i]}")
            print(f"â¡ï¸  ä¸‹ä¸€ä¸ªçŠ¶æ€ (next_obs): \n{replay_buffer.next_observations[i]}")
            print("-" * 80)

    def insert_custom_data(replay_buffer, custom_data, num_writes=1):
        """å‘ ReplayBuffer æ‰‹åŠ¨å†™å…¥æŒ‡å®šæ•°æ®"""
        for _ in range(num_writes):
            index = replay_buffer.pos  # è·å–å½“å‰å†™å…¥ç´¢å¼•
            obs, next_obs, action, reward, done = custom_data

            replay_buffer.observations[index] = np.array(obs)
            replay_buffer.next_observations[index] = np.array(next_obs)
            replay_buffer.actions[index] = np.array(action)
            replay_buffer.rewards[index] = np.array(reward)
            replay_buffer.dones[index] = np.array(done)

            # æ›´æ–° ReplayBuffer ç´¢å¼•
            replay_buffer.pos += 1
            if replay_buffer.pos >= replay_buffer.buffer_size:
                replay_buffer.full = True
                replay_buffer.pos = 0  # é‡æ–°å¼€å§‹è¦†ç›–æ—§æ•°æ®

            print(f"âœ… å·²å†™å…¥æ–°æ•°æ®åˆ°ç´¢å¼• {index}")

        return replay_buffer

    # **ğŸ”¹ 1. æŒ‡å®šè¦åŠ è½½çš„ ReplayBuffer æ–‡ä»¶**
    file_path = "logs_norm/rl_model_norm_replay_buffer_700_steps_2.pkl"
    replay_buffer = load_replay_buffer(file_path)

    # **ğŸ”¹ 2. æŸ¥çœ‹ ReplayBuffer æœ€æ–°çš„ 3 ç»„æ•°æ®**
    print("ğŸ“Œ åŠ è½½ ReplayBufferï¼ŒæŸ¥çœ‹æœ€æ–° 3 ç»„æ•°æ®:")
    print_latest_samples(replay_buffer, num_samples=3)

    # **ğŸ”¹ 3. äººä¸ºå†™å…¥æ–°æ•°æ®ï¼ˆæŒ‡å®šå†™å…¥æ¬¡æ•°ï¼‰**
    custom_data = (
        [[0.5, 1.751, 0.476]],  # è§‚æµ‹å€¼ obs
        [[0.5, 1.621, 0.401]],  # ä¸‹ä¸€ä¸ªè§‚æµ‹å€¼ next_obs
        [[-1, -1, -0.1, 1]],  # åŠ¨ä½œ action
        [0.70243],  # å¥–åŠ± reward
        [0]  # æ˜¯å¦ç»ˆæ­¢ done
    )
    num_writes = 80  # æŒ‡å®šå†™å…¥æ¬¡æ•°
    replay_buffer = insert_custom_data(replay_buffer, custom_data, num_writes=num_writes)

    # **ğŸ”¹ 4. ä¿å­˜æ›´æ–°åçš„ ReplayBuffer**
    save_replay_buffer(replay_buffer, file_path)

    # **ğŸ”¹ 5. é‡æ–°åŠ è½½å¹¶æŸ¥çœ‹æœ€æ–°çš„ 3 ç»„æ•°æ®ï¼Œç¡®ä¿å†™å…¥æˆåŠŸ**
    updated_replay_buffer = load_replay_buffer(file_path.replace(".pkl", "_updated.pkl"))
    print("\nğŸ“Œ å†™å…¥å ReplayBufferï¼ŒæŸ¥çœ‹æœ€æ–° 3 ç»„æ•°æ®:")
    print_latest_samples(updated_replay_buffer, num_samples=3)

if run == 'merge':

    def load_replay_buffer(file_path):
        """åŠ è½½ ReplayBuffer æ–‡ä»¶"""
        with open(file_path, "rb") as f:
            return pickle.load(f)

    def print_latest_samples(replay_buffer, file_name, num_samples=3):
        """æ‰“å° ReplayBuffer ä¸­æœ€æ–°çš„ num_samples ç»„æ•°æ®"""
        max_index = min(num_samples, replay_buffer.size())
        print(f"\n{file_name} å­˜å‚¨æ•°é‡: {replay_buffer.size()}")
        print(f"å½“å‰å­˜å‚¨ä½ç½®: {replay_buffer.pos}")
        print(f"æ˜¯å¦å·²æ»¡: {replay_buffer.full}")

        start_index = max(replay_buffer.pos - max_index, 0)
        for i in range(start_index, replay_buffer.pos):
            print(f"\n æ ·æœ¬ç´¢å¼• {i}:")
            print(f"çŠ¶æ€ (obs): \n{replay_buffer.observations[i]}")
            print(f"åŠ¨ä½œ (action): \n{replay_buffer.actions[i]}")
            print(f"å¥–åŠ± (reward): \n{replay_buffer.rewards[i]}")
            print(f"æ˜¯å¦ç»ˆæ­¢ (done): \n{replay_buffer.dones[i]}")
            print(f"ä¸‹ä¸€ä¸ªçŠ¶æ€ (next_obs): \n{replay_buffer.next_observations[i]}")
            print("-" * 80)

    # è·¯å¾„é…ç½®
    target_file = "logs_norm/rl_model_norm_replay_buffer_700_steps-0.pkl"
    injection_files = [
        'logs_norm/rl_model_norm_replay_buffer_700_steps-5.pkl',
        'logs_norm/rl_model_norm_replay_buffer_700_steps-10.pkl',
        'logs_norm/rl_model_norm_replay_buffer_700_steps-15.pkl',
        'logs_norm/rl_model_norm_replay_buffer_700_steps-30.pkl',
        'logs_norm/rl_model_norm_replay_buffer_600_steps-45.pkl'
    ]

    # åŠ è½½ç›®æ ‡ç¼“å†²æ± 
    replay_buffer_target = load_replay_buffer(target_file)
    buffer_size = replay_buffer_target.buffer_size
    pos_target = replay_buffer_target.pos

    # é€ä¸ªæ³¨å…¥
    for injection_file in injection_files:
        replay_buffer_inject = load_replay_buffer(injection_file)
        inject_size = replay_buffer_inject.size()

        print(f"\næ³¨å…¥æ–‡ä»¶: {injection_file} æœ€æ–° 3 ç»„æ•°æ®:")
        print_latest_samples(replay_buffer_inject, injection_file, num_samples=3)

        num_to_add = min(buffer_size, inject_size)
        indices_to_copy = np.arange(0, num_to_add)

        for idx in indices_to_copy:
            replay_buffer_target.observations[pos_target] = replay_buffer_inject.observations[idx]
            replay_buffer_target.next_observations[pos_target] = replay_buffer_inject.next_observations[idx]
            replay_buffer_target.actions[pos_target] = replay_buffer_inject.actions[idx]
            replay_buffer_target.rewards[pos_target] = replay_buffer_inject.rewards[idx]
            replay_buffer_target.dones[pos_target] = replay_buffer_inject.dones[idx]

            pos_target += 1
            if pos_target >= buffer_size:
                pos_target = 0
                replay_buffer_target.full = True

    # æ›´æ–°ç¼“å†²æ± çš„æŒ‡é’ˆä½ç½®
    replay_buffer_target.pos = pos_target

    # ä¿å­˜æ–°æ–‡ä»¶
    updated_filename = target_file.replace(".pkl", "_merged.pkl")
    with open(updated_filename, "wb") as f:
        pickle.dump(replay_buffer_target, f)

    print(f"\nåˆå¹¶å®Œæˆï¼æ–°ç¼“å†²æ± å·²ä¿å­˜è‡³ {updated_filename}")
    print(f" ç›®æ ‡ç¼“å†²æ± æ›´æ–°åå­˜å‚¨ä½ç½®: {replay_buffer_target.pos}")

    # æ£€æŸ¥åˆå¹¶åçš„å†…å®¹
    updated_replay_buffer = load_replay_buffer(updated_filename)
    print("\næ›´æ–°åçš„æœ€æ–° 3 ç»„æ•°æ®:")
    print_latest_samples(updated_replay_buffer, updated_filename, num_samples=3)

if run == 'newinject':

    def load_replay_buffer(file_path):
        """åŠ è½½ ReplayBuffer æ–‡ä»¶"""
        with open(file_path, "rb") as f:
            return pickle.load(f)

    def print_latest_samples(replay_buffer, file_name, num_samples=3):
        """æ‰“å° ReplayBuffer ä¸­æœ€æ–°çš„ num_samples ç»„æ•°æ®"""
        max_index = min(num_samples, replay_buffer.size())
        print(f"\n{file_name} å­˜å‚¨æ•°é‡: {replay_buffer.size()}")
        print(f"å½“å‰å­˜å‚¨ä½ç½®: {replay_buffer.pos}")
        print(f"æ˜¯å¦å·²æ»¡: {replay_buffer.full}")

        start_index = max(replay_buffer.pos - max_index, 0)
        for i in range(start_index, replay_buffer.pos):
            print(f"\n æ ·æœ¬ç´¢å¼• {i}:")
            print(f"çŠ¶æ€ (obs): \n{replay_buffer.observations[i]}")
            print(f"åŠ¨ä½œ (action): \n{replay_buffer.actions[i]}")
            print(f"å¥–åŠ± (reward): \n{replay_buffer.rewards[i]}")
            print(f"æ˜¯å¦ç»ˆæ­¢ (done): \n{replay_buffer.dones[i]}")
            print(f"ä¸‹ä¸€ä¸ªçŠ¶æ€ (next_obs): \n{replay_buffer.next_observations[i]}")
            print("-" * 80)

    injection_file = "logs_norm/rl_model_norm_replay_buffer_700_steps-5.pkl"
    target_file = "logs_norm/rl_model_norm_replay_buffer_700_steps.pkl"

    replay_buffer_inject = load_replay_buffer(injection_file)
    replay_buffer_target = load_replay_buffer(target_file)

    print("æ³¨å…¥æ–‡ä»¶çš„æœ€æ–° 3 ç»„æ•°æ®:")
    print_latest_samples(replay_buffer_inject, injection_file, num_samples=3)

    buffer_size = replay_buffer_target.buffer_size
    pos_target = replay_buffer_target.pos
    inject_size = replay_buffer_inject.size()

    num_to_add = min(700, inject_size)
    indices_to_copy = np.arange(0, num_to_add)

    for idx in indices_to_copy:
        replay_buffer_target.observations[pos_target] = replay_buffer_inject.observations[idx]
        replay_buffer_target.next_observations[pos_target] = replay_buffer_inject.next_observations[idx]
        replay_buffer_target.actions[pos_target] = replay_buffer_inject.actions[idx]
        replay_buffer_target.rewards[pos_target] = replay_buffer_inject.rewards[idx]
        replay_buffer_target.dones[pos_target] = replay_buffer_inject.dones[idx]

        pos_target += 1
        if pos_target >= buffer_size:
            pos_target = 0
            replay_buffer_target.full = True

    replay_buffer_target.pos = pos_target

    updated_filename = target_file.replace(".pkl", "_updated.pkl")
    with open(updated_filename, "wb") as f:
        pickle.dump(replay_buffer_target, f)

    print(f"\nåˆå¹¶å®Œæˆï¼æ–°ç¼“å†²æ± å·²ä¿å­˜è‡³ {updated_filename}")
    print(f" ç›®æ ‡ç¼“å†²æ± æ›´æ–°åå­˜å‚¨ä½ç½®: {replay_buffer_target.pos}")

    updated_replay_buffer = load_replay_buffer(updated_filename)
    print("\næ›´æ–°åçš„æœ€æ–° 3 ç»„æ•°æ®:")
    print_latest_samples(updated_replay_buffer, updated_filename, num_samples=3)